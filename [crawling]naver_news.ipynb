{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd06f92687704b81907b18d740b3198fc1d5cc1b59ca0cd7f9b010822fc052febd3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/1 [01:13<?, ?it/s]\n",
      "브라우저를 실행시킵니다(자동 제어)\n",
      "\n",
      "설정한 언론사를 선택합니다.\n",
      "\n",
      "연합뉴스\n",
      "\n",
      "크롤링을 시작합니다.\n",
      " 90%|█████████ | 9/10 [01:16<00:08,  8.47s/it]\n",
      "\n",
      "브라우저를 종료합니다.\n",
      "====================================================================================================\n",
      "\n",
      "데이터프레임 변환\n",
      "\n",
      "엑셀 저장 완료 | 경로 : c:\\Users\\lsw91\\Desktop\\workspace_py\\py_study\\연합뉴스뉴스_본문_10개_LG 디스플레이_2021-04-14_00시31분.csv\n",
      "\n",
      "====================================================================================================\n",
      "결과물의 일부\n",
      "브라우저를 실행시킵니다(자동 제어)\n",
      "\n",
      "설정한 언론사를 선택합니다.\n",
      "\n",
      "연합뉴스\n",
      "\n",
      "크롤링을 시작합니다.\n",
      " 10%|█         | 1/10 [00:21<03:09, 21.06s/it]\n",
      "\n",
      "except\n",
      "브라우저를 종료합니다.\n",
      "====================================================================================================\n",
      "\n",
      "데이터프레임 변환\n",
      "\n",
      "엑셀 저장 완료 | 경로 : c:\\Users\\lsw91\\Desktop\\workspace_py\\py_study\\연합뉴스뉴스_본문_2개_엘지 디스플레이_2021-04-14_00시31분.csv\n",
      "\n",
      "====================================================================================================\n",
      "결과물의 일부\n",
      "브라우저를 실행시킵니다(자동 제어)\n",
      "\n",
      "설정한 언론사를 선택합니다.\n",
      "\n",
      "매일경제\n",
      "\n",
      "크롤링을 시작합니다.\n",
      "100%|██████████| 10/10 [00:13<00:00,  1.40s/it]\n",
      "\n",
      "브라우저를 종료합니다.\n",
      "====================================================================================================\n",
      "\n",
      "데이터프레임 변환\n",
      "\n",
      "엑셀 저장 완료 | 경로 : c:\\Users\\lsw91\\Desktop\\workspace_py\\py_study\\매일경제뉴스_본문_11개_LG 디스플레이_2021-04-14_00시31분.csv\n",
      "\n",
      "====================================================================================================\n",
      "결과물의 일부\n",
      "브라우저를 실행시킵니다(자동 제어)\n",
      "\n",
      "설정한 언론사를 선택합니다.\n",
      "\n",
      "MBC\n",
      "\n",
      "크롤링을 시작합니다.\n",
      " 90%|█████████ | 9/10 [00:03<00:00,  2.65it/s]\n",
      "\n",
      "브라우저를 종료합니다.\n",
      "====================================================================================================\n",
      "\n",
      "데이터프레임 변환\n",
      "\n",
      "엑셀 저장 완료 | 경로 : c:\\Users\\lsw91\\Desktop\\workspace_py\\py_study\\MBC뉴스_본문_10개_LG 디스플레이_2021-04-14_00시31분.csv\n",
      "\n",
      "====================================================================================================\n",
      "결과물의 일부\n",
      "브라우저를 실행시킵니다(자동 제어)\n",
      "\n",
      "설정한 언론사를 선택합니다.\n",
      "\n",
      "SBS\n",
      "\n",
      "크롤링을 시작합니다.\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.51s/it]\n",
      "\n",
      "브라우저를 종료합니다.\n",
      "====================================================================================================\n",
      "\n",
      "데이터프레임 변환\n",
      "\n",
      "엑셀 저장 완료 | 경로 : c:\\Users\\lsw91\\Desktop\\workspace_py\\py_study\\SBS뉴스_본문_11개_LG 디스플레이_2021-04-14_00시31분.csv\n",
      "\n",
      "====================================================================================================\n",
      "결과물의 일부\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "from pandas import DataFrame\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pickle, progressbar, json, glob, time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def crawling_main_text(url):\n",
    "\n",
    "    req = requests.get(url)\n",
    "    req.encoding = None\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    \n",
    "    # 연합뉴스\n",
    "    #if ('://yna' in url) | ('app.yonhapnews' in url): \n",
    "    if ('yna' in url) | ('app.yonhapnews' in url):    \n",
    "        main_article = soup.find('div', {'class':'story-news article'})\n",
    "        if main_article == None:\n",
    "            main_article = soup.find('div', {'class' : 'article-txt'})\n",
    "            \n",
    "        text = main_article.text\n",
    "        \n",
    "    # MBC \n",
    "    elif '//imnews.imbc' in url: \n",
    "        text = soup.find('div', {'itemprop' : 'articleBody'}).text\n",
    "        \n",
    "    # 매일경제(미라클), req.encoding = None 설정 필요\n",
    "    elif 'mirakle.mk' in url:\n",
    "        text = soup.find('div', {'class' : 'view_txt'}).text\n",
    "        \n",
    "    # 매일경제, req.encoding = None 설정 필요\n",
    "    elif 'mk.co' in url:\n",
    "        text = soup.find('div', {'class' : 'art_txt'}).text\n",
    "        \n",
    "    # SBS\n",
    "    elif 'news.sbs' in url:\n",
    "        text = soup.find('div', {'itemprop' : 'articleBody'}).text\n",
    "    \n",
    "    # KBS\n",
    "    elif 'news.kbs' in url:\n",
    "        text = soup.find('div', {'id' : 'cont_newstext'}).text\n",
    "        \n",
    "    # JTBC\n",
    "    elif 'news.jtbc' in url:\n",
    "        text = soup.find('div', {'class' : 'article_content'}).text\n",
    "        \n",
    "    # 그 외\n",
    "    else:\n",
    "        text = None\n",
    "        \n",
    "    return text.replace('\\n','').replace('\\r','').replace('<br>','').replace('\\t','')\n",
    "    \n",
    "\n",
    "def open_browser(press_list, query):\n",
    "    print('브라우저를 실행시킵니다(자동 제어)\\n')\n",
    "    chrome_path = 'C:/chromedriver/chromedriver.exe'\n",
    "    browser = webdriver.Chrome(chrome_path)\n",
    "\n",
    "    news_url = 'https://search.naver.com/search.naver?where=news&query={}'.format(query)\n",
    "    browser.get(news_url)\n",
    "    time.sleep(sleep_sec)\n",
    "    ######### 언론사 선택 및 confirm #####################\n",
    "    print('설정한 언론사를 선택합니다.\\n')\n",
    "    search_opt_box = browser.find_element_by_xpath('//*[@id=\"search_option_button\"]')\n",
    "    search_opt_box.click()\n",
    "    time.sleep(0.02)\n",
    "\n",
    "    # 언론사 선택하는 바를 활성화\n",
    "    tablist_box = browser.find_element_by_xpath('//div[@class=\"snb_inner\"]/ul[@role=\"tablist\" and @class=\"option_menu\"]')\n",
    "\n",
    "    tablist_elem_list = tablist_box.find_elements_by_xpath('./li[@role=\"presentation\"]')\n",
    "    press_box = [t for t in tablist_elem_list if t.text == '언론사'][0].find_element_by_xpath('./a')\n",
    "    press_box.click()\n",
    "\n",
    "\n",
    "    # 언론사 종류 하나씩 선택\n",
    "    actived_press_frame = browser.find_element_by_xpath('.//div[@class=\"snb_itembox lst_press _search_option_press_\"]')\n",
    "    total_press_box = actived_press_frame.find_element_by_xpath('./div[@class=\"group_sort type_press _group_by_press_\"]')\n",
    "\n",
    "    # 언론사 종류를 선택하는 버튼이 담긴 박스\n",
    "    press_cat_active_button = total_press_box.find_elements_by_xpath('.//a[@role=\"tab\" and @class=\"item _tab_filter_\"]') # 언론사 종류 하나씩 버튼\n",
    "    press_cat_active_button_dict = dict(zip([t.text for t in press_cat_active_button], press_cat_active_button)) # 언론사 종류 이름 : 언론사 종류 활성화 버튼\n",
    "\n",
    "    # 밑에 각 언론사 종류별 개별 언론사가 담겨있는 박스들\n",
    "    each_press_box_list = total_press_box.find_elements_by_xpath('.//div[@class=\"scroll_area _panel_filter_\"]')\n",
    "\n",
    "    # 1. 언론사 종류 1개 선택\n",
    "    # 2. 선택한 언론사 종류에 해당하는 개별 언론사 중 크롤링할 언론사에 포함되는 것 체크 \n",
    "    for idx, press_cat_name in enumerate(press_cat_active_button_dict.keys()):\n",
    "        #하나의 언론사 종류를 클릭해서 활성화시킴\n",
    "        press_cat_active_button_dict[press_cat_name].click()\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "        # 선택한 언론사 종류 안의 개별 언론사가 담긴 박스\n",
    "        each_press_box = each_press_box_list[idx].find_element_by_xpath('./div[@class=\"select_item\"]')\n",
    "        # 개별 언론사의 이름\n",
    "        each_press_title_list = [ep.get_attribute('title') for ep in each_press_box.find_elements_by_xpath('.//label')]\n",
    "        # 개별 언론사 체크 박스\n",
    "        each_press_input_list = each_press_box.find_elements_by_xpath('.//input')\n",
    "        \n",
    "\n",
    "        # 딕셔너리(개별 언론사 이름 : 개별 언론사 체크 박스)\n",
    "        each_press_title_input_dict = dict(zip(each_press_title_list, each_press_input_list))\n",
    "        # 추출하고 싶은 언론사 존재 시 체크박스 클릭\n",
    "        for title in [tit for tit in each_press_title_input_dict.keys() if tit in press_list]:\n",
    "            print(title)\n",
    "            each_press_title_input_dict[title].click()\n",
    "\n",
    "\n",
    "    # 확인 버튼\n",
    "    confirm_buttons = actived_press_frame.find_element_by_xpath('./span[@class=\"btn_inp\"]').find_elements_by_xpath('.//button')\n",
    "    ok_button = [c for c in confirm_buttons if c.text == '확인'][0]\n",
    "    ok_button.click()\n",
    "\n",
    "    return browser\n",
    "\n",
    "def start_crawling(browser, news_num):\n",
    "    try:\n",
    "        print('\\n크롤링을 시작합니다.')\n",
    "        time.sleep(sleep_sec)\n",
    "        # ####동적 제어로 페이지 넘어가며 크롤링\n",
    "        news_dict = {}\n",
    "        idx = 1\n",
    "        cur_page = 1\n",
    "\n",
    "        pbar = tqdm(total=news_num)\n",
    "            \n",
    "        while idx < news_num:\n",
    "\n",
    "            table = browser.find_element_by_xpath('//ul[@class=\"list_news\"]')\n",
    "            li_list = table.find_elements_by_xpath('./li[contains(@id, \"sp_nws\")]')\n",
    "            area_list = [li.find_element_by_xpath('.//div[@class=\"news_area\"]') for li in li_list]\n",
    "            a_list = [area.find_element_by_xpath('.//a[@class=\"news_tit\"]') for area in area_list]\n",
    "\n",
    "            for n in a_list[:min(len(a_list), news_num-idx+1)]:\n",
    "                n_url = n.get_attribute('href')\n",
    "                \n",
    "                try:\n",
    "                    news_dict[idx] = {'title' : n.get_attribute('title'), \n",
    "                                    'url' : n_url,\n",
    "                                    'text' : crawling_main_text(n_url)}\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                idx += 1\n",
    "                pbar.update(1)\n",
    "                \n",
    "            if idx < news_num:\n",
    "                cur_page +=1\n",
    "\n",
    "                pages = browser.find_element_by_xpath('//div[@class=\"sc_page_inner\"]')\n",
    "\n",
    "                try:\n",
    "                    next_page_url = [p for p in pages.find_elements_by_xpath('.//a') if p.text == str(cur_page)][0].get_attribute('href')\\\n",
    "                \n",
    "                except:\n",
    "                    pbar.close()\n",
    "        \n",
    "                    print('\\nexcept(1)\\n브라우저를 종료합니다.\\n' + '=' * 100)\n",
    "                    time.sleep(0.7)\n",
    "                    browser.close()\n",
    "                    break\n",
    "\n",
    "                browser.get(next_page_url)\n",
    "                time.sleep(sleep_sec)\n",
    "            else:\n",
    "                pbar.close()\n",
    "                \n",
    "                print('\\n브라우저를 종료합니다.\\n' + '=' * 100)\n",
    "                time.sleep(0.7)\n",
    "                browser.close()\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(\"\\nfailed : \",e )\n",
    "    finally:\n",
    "        return news_dict\n",
    "\n",
    "def make_csv(news_dict, press, query):\n",
    "    print('\\n데이터프레임 변환\\n')\n",
    "    idx=len(news_dict)\n",
    "    news_df = DataFrame(news_dict).T\n",
    "\n",
    "    folder_path = os.getcwd()\n",
    "    xlsx_file_name = '{}뉴스_본문_{}개_{}_{}.csv'.format(press, idx, query, date)\n",
    "\n",
    "    news_df.to_csv(xlsx_file_name, encoding='utf-8')\n",
    "\n",
    "    print('엑셀 저장 완료 | 경로 : {}\\\\{}\\n'.format(folder_path, xlsx_file_name))\n",
    "\n",
    "    os.startfile(folder_path)\n",
    "\n",
    "    print('=' * 100 + '\\n결과물의 일부')\n",
    "    news_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ###### 날짜 저장 ##########\n",
    "    date = str(datetime.now())\n",
    "    date = date[:date.rfind(':')].replace(' ', '_')\n",
    "    date = date.replace(':','시') + '분'\n",
    "\n",
    "    sleep_sec = 0.5\n",
    "\n",
    "    in_data = pd.read_csv('input.csv', index_col='press')\n",
    "    for index, row in in_data.iterrows():\n",
    "        press_list=index\n",
    "        query=row.key_word\n",
    "        news_num=row.search_count\n",
    "\n",
    "        # open chrome and selec press list\n",
    "        browser=open_browser(press_list, query)\n",
    "        # start crawling\n",
    "        news_dict, idx = start_crawling(browser, news_num)\n",
    "        # make csv using crawling results\n",
    "        make_csv(news_dict, press_list, query)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('input.csv',index_col='press')\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "#for index, row in df.iterrows():\n",
    "#    print(\"press : {}, key_word : {}, count : {}\".format(index, row.key_word, row.search_count))    \n",
    "    "
   ]
  }
 ]
}